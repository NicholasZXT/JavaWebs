# -------- Spark 2.x \u7684\u65E5\u5FD7\u914D\u7F6E\u6587\u4EF6\uFF1ASlf4j + Log4j 1.2.17 \u7EC4\u5408 ---------
# \u9700\u8981\u4FDD\u8BC1 Slf4j \u7ED1\u5B9A\u7684\u540E\u7AEF\u662F Log4j\uFF0C\u5982\u679CPOM\u91CC\u8FD8\u5F15\u5165\u4E86 Logback \u5E76\u4E14 Logback \u7684\u4F9D\u8D56\u8FD8\u5728 Spark\u76F8\u5173\u4F9D\u8D56\u524D\u9762\u7684\u8BDD
# \u90A3\u4E48 Slf4j \u5F88\u53EF\u80FD\u4F1A\u9009\u62E9 Logback (logback-classic) \u4F5C\u4E3A\u540E\u7AEF\uFF0C\u5BFC\u81F4\u8FD9\u91CC\u65E5\u5FD7\u914D\u7F6E\u9879\u5728\u672C\u5730\u8C03\u8BD5 Spark \u65F6\u5E76\u4E0D\u80FD\u751F\u6548\u3002
# Set everything to be logged to the console
# log4j.rootCategory=DEBUG, console
 log4j.rootCategory=INFO, console
#log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=[%d{yyyy-MM-dd HH:mm:ss}][%p][%c]: %m%n

# Set the default spark-shell log level to WARN. When running the spark-shell, the
# log level for this class is used to overwrite the root logger's log level, so that
# the user can have different defaults for the shell and regular Spark apps.
log4j.logger.org.apache.spark.repl.Main=WARN

# Settings to quiet third party logs that are too verbose
log4j.logger.org.apache.spark.SecurityManager=ERROR
log4j.logger.org.apache.spark.storage=WARN
log4j.logger.org.apache.spark.SparkEnv=WARN
log4j.logger.org.spark_project.jetty=WARN
log4j.logger.org.apache.spark.scheduler=WARN
log4j.logger.org.apache.spark.sql.internal=WARN
log4j.logger.org.apache.spark.ContextCleaner=WARN
log4j.logger.org.apache.spark.executor.Executor=WARN
log4j.logger.org.apache.spark.sql.catalyst=WARN
log4j.logger.org.apache.spark.sql.execution.streaming=WARN
log4j.logger.org.apache.spark.sql.execution.datasources=WARN
#log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
#log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
#log4j.logger.org.apache.parquet=ERROR
#log4j.logger.parquet=ERROR

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR

# Flink
log4j.logger.org.apache.flink.runtime=WARN
#log4j.logger.org.apache.flink.runtime.taskexecutor=WARN
#log4j.logger.org.apache.flink.runtime.metrics=WARN
#log4j.logger.org.apache.flink.runtime.rpc=WARN
#log4j.logger.org.apache.flink.runtime.security=WARN
#log4j.logger.org.apache.flink.runtime.blob=WARN
#log4j.logger.org.apache.flink.runtime.io.network=WARN